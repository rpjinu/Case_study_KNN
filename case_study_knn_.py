# -*- coding: utf-8 -*-
"""Case Study - KNN  .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aRsBXJuBywI5pVfESURljzCApWobJj8x

## Problem Statement

One of the companies maintains a database of the employees and their details. Based on this dataset the requirememt is to create the classification model using K Nearest Neighbour to predict if the employee will leave the company or not (attrition rate). There are around 1400+ records using this we need to create the model after breaking the data into test and train.

**Data Dictionary**

**Age** - Age of the employee

**Attrition** - Dependent variable, if employee will leave or not

**BusinessTravel** - How frequently employee travels for business       

**DailyRate** - Daily rate of an employee                    

**Department** - Which department employee belongs to                  

**DistanceFromHome** - How far employee stays from the office             

**EducationField** - Education field of the employee              

**EnvironmentSatisfaction** -  Rating of satisfaction     

**Gender** -  Gender of an employee                     

**HourlyRate** -  Hourly Rate of an employee                  

**JobRole** - Job role of an employee                     

**JobSatisfaction** - Job satisfaction rating              

**MaritalStatus** - Marital status of an employee               

**MonthlyIncome** - Monthly Income of an employee               

**MonthlyRate** -  Monthly rate of an employee                 

**NumCompaniesWorked** - Number of companies employee worked on                                 

**OverTime** -  Is employee working overtime                   

**PercentSalaryHike** - Percentage salary hike of an employee            

**PerformanceRating** - Performance rating of an employee             

**StandardHours** -  Standard working hours of an employee                            

**TotalWorkingYears** - Total working years of an employee            

**TrainingTimesLastYear** - Total training time        

**WorkLifeBalance** -  Work life balance rating             

**YearsAtCompany** - Years at the current company               

**YearsInCurrentRole** - Years at the current role           

**YearsSinceLastPromotion** - Year since last promotion      

**YearsWithCurrManager** - Years with current manager

# Table of Content

1. **[Import Libraries](#lib)**
2. **[Data Preparation](#prep)**
    - 2.1 - **[Understand the Data](#read)**
    - 2.2 - **[Exploratory Data Analysis](#eda)**
    - 2.3 - **[Missing Value Treatment](#null)**
    - 2.4 - **[Encoding and Feature Scaling](#enc)**
3. **[What is K-Nearest Neighbour](#lr)**
    - 3.1 - **[Geometric Intuition](#gi)**
    - 3.2 - **[How the value of K impacts the model - Underfitting and Overfitting](#mf)**
    - 3.3 - **[How to find the value of K](#sf)**
    - 3.4 - **[Concept of Weighted K-Nearest Neighbour](#wknn)**
4. **[Splitting the data into Train and Test](#sd)**
5. **[Creating the model on training dataset](#model)**
6. **[Run the model on the Test Dataset](#test)**
7. **[Check the accuracy of the model](#acc)**
    - 7.1 - **[Accuracy Score](#accscore)**
    - 7.2 - **[Confusion Matrix](#cm)**
    - 7.3 - **[ROC Curve](#roc)**
    - 7.4 - **[F1 Score](#f1score)**
    - 7.5 - **[Log Loss](#logloss)**
8. **[Comparing the Training and Testing Accuracies](#overunder)**
9. **[Applying K-Fold Cross Validation to find the best value of K](#kfcv)**
10. **[Applying Weighted K-Nearest Neighbour](#wknn)**

<a id="lib"></a>
# 1. Import Libraries
"""

#Importing the libraries which will be helpful for the data analysis.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""<a id="prep"></a>
# 2. Data Preparation
"""

#Importing the dataset which we will use for the modelling
dataset = pd.read_csv('HR-Employee-Attrition.csv')

"""<a id="read"></a>
# 2.1. Understand the Data
"""

#Here are the few commands which will help us to understand the basic data
#The info command will help us to understand the different columns present in the dataset and its datatype
dataset.info()

#Len command will help us understand the total number of records present in the dataset
len(dataset)

#.columns command will help us understand the columns present in the dataset
dataset.columns

#The below command will help us understand the total number of columns present in the dataset
len(dataset.columns)

"""<a id="eda"></a>
# 2.2. Exploratory Data Analysis
"""

plt.figure(figsize=(15,2))
sns.pairplot(dataset[['Age',  'DailyRate','DistanceFromHome', 'EnvironmentSatisfaction','HourlyRate','Attrition']],hue='Attrition')

plt.figure(figsize=(15,2))
sns.pairplot(dataset[['JobSatisfaction','MonthlyIncome','MonthlyRate', 'NumCompaniesWorked',    'PercentSalaryHike','Attrition']],hue='Attrition')

sns.pairplot(dataset[['PerformanceRating', 'StandardHours',
       'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance',
       'Attrition']],hue='Attrition')

sns.pairplot(dataset[['YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion',
       'YearsWithCurrManager','Attrition']],hue='Attrition')

"""**From the above charts where we have used the numerical features we are not seeing any significant patterns in the dataset**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x = 'BusinessTravel',hue='Attrition')
plt.show()

"""**From the above chart we can say that employees who are travelling rare are more prone to leave the company**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x = 'Department',hue='Attrition')
plt.show()

"""**From the above chart we can say that the employees who are working in the R&D Department are more willing to leave the company**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x = 'EducationField',hue='Attrition')
plt.show()

"""**From the above chart we can say that the employees who are having a life sciences and Medical background of education are more willing to leave the company**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x = 'Gender',hue='Attrition')
plt.show()

"""**There is no significant insights coming from the gender**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x = 'JobRole',hue='Attrition')
plt.show()

"""**There is no significant insights coming from the Job role**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x = 'MaritalStatus',hue='Attrition')
plt.show()

"""**There is no significant insights coming from the Marital Status**"""

plt.figure(figsize=(15,2))
sns.countplot(data=dataset, x = 'OverTime',hue='Attrition')
plt.show()

"""**Employees who are not doing overtime are more prone to leave the company**

<a id="null"></a>
# 2.3. Missing Value Treatment
"""

#Checking the count of the missing values percentage, there are very few missing values there in the dataset
dataset.isnull().sum()/len(dataset)*100

#Missing Value Imputation - We can impute the missing values using the methods of mean, median and mode based on the various scenarios
#When there is a numerical field we can populate the missing values using mean or median,
#if there are outliers in the dataset we use to populate the missing values using median else mean
#When we want to populate the missing values in the categorial files we go with mode as an option

"""<a id="enc"></a>
# 2.4. Encoding and Feature Scaling
"""

# Separating the numerical and categorical columns
from sklearn.preprocessing import StandardScaler
def data_type(dataset):
    """
    Function to identify the numerical and categorical data columns
    :param dataset: Dataframe
    :return: list of numerical and categorical columns
    """
    numerical = []
    categorical = []
    for i in dataset.columns:
        if dataset[i].dtype == 'int64' or dataset[i].dtype == 'float64':
            numerical.append(i)
        else:
            categorical.append(i)
    return numerical, categorical


numerical, categorical = data_type(dataset)

# Identifying the binary columns and ignoring them from scaling
def binary_columns(df):
    """
    Generates a list of binary columns in a dataframe.
    """
    binary_cols = []
    for col in df.select_dtypes(include=['int', 'float']).columns:
        unique_values = df[col].unique()
        if np.in1d(unique_values, [0, 1]).all():
            binary_cols.append(col)
    return binary_cols

binary_cols = binary_columns(dataset)

# Remove the binary columns from the numerical columns
numerical = [i for i in numerical if i not in binary_cols]

def encoding(dataset, categorical):
    """
    Function to automate the process of encoding the categorical data
    :param dataset: Dataframe
    :param categorical: List of categorical columns
    :return: Dataframe
    """
    for i in categorical:
        dataset[i] = dataset[i].astype('category')
        dataset[i] = dataset[i].cat.codes
    return dataset

dataset = encoding(dataset, categorical)

def feature_scaling(dataset, numerical):
    """
    Function to automate the process of feature scaling the numerical data
    :param dataset: Dataframe
    :param numerical: List of numerical columns
    :return: Dataframe
    """
    sc_x = StandardScaler()
    dataset[numerical] = sc_x.fit_transform(dataset[numerical])
    return dataset

dataset = feature_scaling(dataset, numerical)

dataset

"""<a id="lr"></a>
# 3. What is K-Nearest Neighbours

K-Nearest Neighbour is one of the very simple Machine Learning models used for Classification. In this model machine will try to find the K number of neighbours near to the query point and based on the majority it predicts the output. This algorithm is simple but it is very much  resource exhaustive.

<a id="sf"></a>
# 3.3. How to find the value of K

Using the technique called K-Fold Cross validation technique we will find the best value of K so that the model will neither be overfit nor underfit

<a id="sf"></a>
# 3.4. Concept of Weighted K-Nearest Neighbour

The concept of weighted K-Nearest neighbour is where the importance will be given to the points which are more near to the query points i.e., lesser the distance more the importance given to those points and farther the points lesser the importance given to those points.

<a id="sd"></a>
# 4. Splitting the data into Train and Test
"""

#Splitting all the independent variables in one array
x = dataset.iloc[:,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]].values

#Splitting the dependent variable in one array
y = dataset.iloc[:,1].values

#Splitting the dataset into train and test based on the 70-30 ratio
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30)

"""<a id="model"></a>
# 5. Creating the model on training dataset
"""

#Applying the K-Nearest Neighbour on the training dataset
from sklearn.neighbors import KNeighborsClassifier
knnmodel_ini = KNeighborsClassifier()
knnmodel_ini.fit(x_train,y_train)

"""<a id="test"></a>
# 6. Run the model on the Test Dataset
"""

#Running the model on the test dataset
y_pred_ini = knnmodel_ini.predict(x_test)

"""<a id="acc"></a>
# 7. Check the accuracy of the model

There are various ways to check the accuracy of the classification model we are going to use all the ways to check the accuracies
"""

#Importing all the functions to for checking the accuracies
from sklearn.metrics import classification_report,confusion_matrix,plot_confusion_matrix,plot_precision_recall_curve,plot_roc_curve, accuracy_score, log_loss

"""<a id="accscore"></a>
# 7.1. Accuracy Score
"""

#Using accuracy score we are checking the accuracy on the testing dataset
accuracy_score(y_test,y_pred_ini)

"""<a id="cm"></a>
# 7.2. Confusion Matrix
"""

#Using confusion matrix we are checking the accuracy on the testing dataset
plot_confusion_matrix(knnmodel_ini,x_test,y_test)

"""<a id="roc"></a>
# 7.3. RO Curve
"""

#Using ROC Curve we are checking the accuracy on the testing dataset
plot_roc_curve(knnmodel_ini, x_test, y_test)
plt.title("Plot of ROC Curve for LR Model")
plt.show()

"""<a id="f1score"></a>
# 7.4. F1 Score
"""

#Using F1 Score we are checking the accuracy on the testing dataset
target_names= ["Negative(0)","Positive(1)"]
# Classification Report
print(classification_report(y_test,knnmodel_ini.predict(x_test),target_names=target_names))

"""<a id="logloss"></a>
# 7.5. Log Loss
"""

#Using Logloss we are checking the accuracy on the testing dataset
log_loss(y_test,knnmodel_ini.predict(x_test))

"""<a id="overunder"></a>
# 8. Comparing the Training and Testing Accuracies
"""

#Storing the predicted values of training dataset in y_pred_train
y_pred_train = knnmodel_ini.predict(x_train)

#Checking the accuracy of training dataset
accuracy_score(y_train,y_pred_train)

#Checking the accuracy of testing dataset
accuracy_score(y_test,y_pred_ini)

"""**Conclusion:** As there is very less difference between the accuracy of training and testing dataset we are good to go with the model

<a id="kfcv"></a>
# 9. Applying K-Fold Cross Validation to find the best value of K
"""

#Using K-fold cross validation technique we will find the best value of K
k_value = [1,3,5,7,9,11,13,15,17,19,21]
from sklearn.model_selection import cross_val_score
cv_score = []

for k in k_value:
    knnmodel = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knnmodel, x_train,y_train,cv=3, scoring='accuracy')
    cv_score.append(scores.mean())

cv_score
plt.plot(k_value, cv_score)
plt.show()
#The best value of K is coming out to be 11, we will retrain the model with the value of K as 11

#Applying the K-Nearest Neighbour on the training dataset
from sklearn.neighbors import KNeighborsClassifier
knnmodel_tuned = KNeighborsClassifier(n_neighbors=7)
knnmodel_tuned.fit(x_train,y_train)

#Running the model on the test dataset
y_pred_tuned = knnmodel_tuned.predict(x_test)

#Using accuracy score we are checking the accuracy on the testing dataset
accuracy_score(y_test,y_pred_tuned)

"""<a id="wknn"></a>
# 10. Applying Weighted K-Nearest Neighbour
"""

#Applying the K-Nearest Neighbour on the training dataset
from sklearn.neighbors import KNeighborsClassifier
knnmodel_wt = KNeighborsClassifier(n_neighbors=7,weights='distance')
knnmodel_wt.fit(x_train,y_train)

#Running the model on the test dataset
y_pred_wt = knnmodel_wt.predict(x_test)

#Using accuracy score we are checking the accuracy on the testing dataset
accuracy_score(y_test,y_pred_wt)

# create an empty dataframe to store the scores for various algorithms
from sklearn.metrics import roc_auc_score, precision_score, recall_score, roc_auc_score, f1_score
score_card = pd.DataFrame(columns=['model_name','Accuracy Score','Precision Score','Recall Score','AUC Score','f1 Score'])

# append the result table for all performance scores

def update_score_card(y_test,y_pred,model_name):

    # assign 'score_card' as global variable
    global score_card

    # append the results to the dataframe 'score_card'
    # 'ignore_index = True' do not consider the index labels
    score_card = score_card.append({'model_name':model_name,
                                    'Accuracy Score' : accuracy_score(y_test, y_pred),
                                    'Precision Score': precision_score(y_test, y_pred),
                                    'Recall Score': recall_score(y_test, y_pred),
                                    'AUC Score': roc_auc_score(y_test, y_pred),
                                    'f1 Score': f1_score(y_test, y_pred)},
                                    ignore_index = True)

update_score_card(y_test,y_pred_ini,'initial_model')

update_score_card(y_test,y_pred_tuned,'tuned_model')

update_score_card(y_test,y_pred_wt,'Weighted KNN')

score_card

"""**Interpretation: There is an increase in the accuracy after fine tuning the hyperparameter, however, there is not difference between normal KNN and weighted KNN in the accuracy point of view**"""